# Demo Plan: Monolith vs. Split-Architektur

**Ziel:** Zeigen, dass IPC kein Bottleneck ist und Split-Architekturen Vorteile bei KomplexitÃ¤t, Robustheit und Polyglot-FÃ¤higkeit bieten.

## Use Case: NDJSON Event Pipeline

Viele Events verarbeiten â†’ Parse â†’ Regeln â†’ Aggregation â†’ Output

**Warum dieser Use Case:**

- Realistische "viele Daten" Situation
- Streaming-friendly
- Interne Kommunikation ist dominant (wie in Monolithen)
- IPC-Overhead ist messbar aber nicht dominant
- Recovery/Restart-Szenarien sind Ã¼berzeugend

---

## Architektur-Vergleich

### Monolith (C++ mit std::thread)

```
Main Thread (Orchestrator)
  â”œâ”€ Reader Thread (NDJSON laden, std::ifstream)
  â”œâ”€ Parser Threads (parse + validate, rapidjson/simdjson)
  â”œâ”€ Rules Thread (filter/enrich, custom logic)
  â”œâ”€ Aggregator Thread (counts/windows, std::unordered_map)
  â””â”€ Writer Thread (output, std::ofstream)

Kommunikation: std::queue + std::mutex, std::condition_variable
Problem: Locks Ã¼berall, komplexe Synchronisation, shared state, manuelle Memory-Management

**Warum C++?**
- ReprÃ¤sentiert typischen Legacy-Monolith ("C++ ist nÃ¶tig")
- Performance-Baseline: beste mÃ¶gliche Single-Process Performance
- Zeigt reale KomplexitÃ¤t: Locks, RAII, Thread-Safety, Memory-Leaks
- Macht Vergleich Ã¼berzeugend: "Selbst gegen C++ ist Split kompetitiv"
```

### Split (Services + Broker)

```
Broker
  â”œâ”€ ingest-service (TypeScript) - liest NDJSON, chunked streaming, Node.js streams
  â”œâ”€ parse-service (Rust) - parse + validate, serde_json, maximale Performance
  â”œâ”€ rules-service (Python) - filter/enrich, dynamische Regeln, schnelle Iteration
  â”œâ”€ aggregate-service (Go) - counts/windows, goroutines, efficient concurrency
  â””â”€ sink-service (TypeScript) - write results, metrics reporting

Kommunikation: gRPC/Protobuf
Vorteile: Prozessgrenzen, klare Contracts, polyglot (4 Sprachen!), restartable

Polyglot-Strategie:
- TypeScript: Orchestrierung, I/O-heavy, schnelle Entwicklung
- Rust: CPU-intensive parsing, memory safety, C++-Alternative
- Python: Flexible Businesslogik, schnelle Iteration, groÃŸe Libs
- Go: Concurrency-heavy aggregation, deployment-friendly
```

---

## Implementierungsplan

### Phase 1: Foundation (Proto + Shared Types)

**1.1 Proto Definitions**

```protobuf
// packages/proto/pipeline/v1/pipeline.proto

service Ingest {
  rpc StreamEvents(StreamEventsRequest) returns (stream Event);
  rpc GetStatus(GetStatusRequest) returns (IngestStatus);
}

service Parse {
  rpc ParseEvents(stream Event) returns (stream ParsedEvent);
}

service Rules {
  rpc ApplyRules(stream ParsedEvent) returns (stream EnrichedEvent);
}

service Aggregate {
  rpc Aggregate(stream EnrichedEvent) returns (stream AggregateResult);
}

message Event {
  string raw_json = 1;
  int64 sequence = 2;
}

message ParsedEvent {
  string type = 1;
  string user = 2;
  int64 value = 3;
  int64 timestamp = 4;
  int64 sequence = 5;
}

message EnrichedEvent {
  ParsedEvent event = 1;
  map<string, string> metadata = 2;
  bool passed_rules = 3;
}

message AggregateResult {
  string key = 1;
  int64 count = 2;
  int64 sum = 3;
  double avg = 4;
}
```

**1.2 Shared Package**

```typescript
// packages/pipeline-common/src/types.ts

export interface EventRecord {
  ts: string
  type: 'click' | 'view' | 'purchase'
  user: string
  value: number
  metadata?: Record<string, unknown>
}

export interface PipelineMetrics {
  processedEvents: number
  invalidEvents: number
  throughputPerSec: number
  latencyP50Ms: number
  latencyP95Ms: number
  latencyP99Ms: number
  cpuPercent: number
  memoryMB: number
}

export interface PipelineConfig {
  mode: 'monolith' | 'split'
  inputFile: string
  outputFile: string
  batchSize: number
  workerCount?: number // monolith only
}
```

**1.3 Contract Semantics (Ã¼bergreifend)**

- Ordering: garantiert pro `sequence`/`key` oder nur best-effort?
- Delivery: at-least-once vs exactly-once (und was ist im Demo-Context akzeptabel)
- Idempotenz/Dedup: wie werden Retries gehandhabt, welche IDs sind stabil
- Ack/Retry Regeln: max retries, backoff, retry budget

**1.4 Schema Evolution**

- Proto-Versionierung (v1/v2) + KompatibilitÃ¤tsregeln
- CI-Check fÃ¼r breaking changes (Protos)
- Beispiel fÃ¼r additive Ã„nderungen + Deprecation-Policy

---

### Phase 2: Test Data Generator

**2.1 Generator Tool**

```typescript
// apps/event-generator/src/generator.ts

export interface GeneratorConfig {
  eventCount: number
  outputFile: string
  userCount: number
  eventTypes: string[]
  seed?: number
}

// Generiert z.B. 5M, 10M, 50M Events
// Reproduzierbar mit seed
// Verschiedene Verteilungen (uniform, zipf, burst)
```

**Beispiel Output:**

```json
{"ts":"2026-01-30T10:00:00Z","type":"click","user":"u1234","value":42}
{"ts":"2026-01-30T10:00:01Z","type":"view","user":"u5678","value":1}
...
```

---

### Phase 3: Monolith Implementation (C++)

**3.1 Structure**

```
apps/event-pipeline-monolith/
  CMakeLists.txt
  src/
    main.cpp              # CLI entry, orchestration
    threads/
      reader.cpp/.hpp     # Read NDJSON file (std::ifstream)
      parser.cpp/.hpp     # Parse + validate (rapidjson/simdjson)
      rules.cpp/.hpp      # Filter/enrich
      aggregator.cpp/.hpp # Count/sum/avg (std::unordered_map + mutex)
      writer.cpp/.hpp     # Output results
    coordinator.cpp/.hpp  # Thread management + queues
    metrics.cpp/.hpp      # Collect metrics (atomics)
    queue.hpp             # Thread-safe queue (std::queue + mutex/cv)
```

**3.2 Thread Communication**

- `std::queue<T>` + `std::mutex` + `std::condition_variable` fÃ¼r Event-Passing
- `std::atomic` fÃ¼r Metrics (lock-free counters)
- Backpressure via queue size limits + cv.wait()
- RAII guards (`std::lock_guard`, `std::unique_lock`) Ã¼berall

**3.3 Key Implementation Points**

- Locks fÃ¼r shared state (aggregation maps): `std::shared_mutex` (reader/writer lock)
- Komplexe Shutdown-Logik: `std::atomic<bool> shutdown_flag`, join() alle threads
- Error handling: try/catch + error queues, keine exceptions Ã¼ber Thread-Grenzen
- Memory management: smart pointers (`std::unique_ptr`, `std::shared_ptr`), aber auch manuelle Cleanup-Logik
- **Typische C++ Probleme zeigen:** Race conditions, Deadlocks, Memory leaks (falls vorhanden), komplexe Debugging

**3.4 Build & Dependencies**

- CMake + vcpkg/conan fÃ¼r Dependencies (rapidjson, CLI11, fmt)
- Compiler: clang++ oder g++ mit C++17/20
- Sanitizers: -fsanitize=thread fÃ¼r Tests (zeigt Race conditions)
- Profiling: perf, Instruments (zeigt Lock-Contention)

---

### Phase 4: Split Implementation

**4.1 Services**

**apps/ingest-service/** (TypeScript)

- Liest NDJSON file, streamt in Chunks
- Implementiert `Ingest` gRPC service
- Rate limiting / backpressure
- Health checks

**apps/parse-service/** (Rust)

- EmpfÃ¤ngt Event stream
- Parse JSON â†’ structured data (serde_json)
- Validierung (type-safe)
- Schnell + memory-efficient
- **Sprach-Rationale:** Rust zeigt maximale Performance bei JSON-Parsing (serde), memory safety ohne GC, und ist ein starkes Gegenargument zu "nur C++ ist schnell"

**apps/rules-service/** (Python)

- EmpfÃ¤ngt ParsedEvent stream
- Wendet Regeln an (z.B. filter by type, enrich with geo data)
- Dynamische Regel-Engine (eval/AST oder rule DSL)
- Demonstriert: "Businesslogik muss nicht C++"
- **Sprach-Rationale:** Python zeigt FlexibilitÃ¤t + schnelle Iteration bei Regeln/Heuristiken, groÃŸe Lib-Ecosystem (pandas, numpy falls benÃ¶tigt), und dass Performance-kritische Teile isoliert sein kÃ¶nnen

**apps/aggregate-service/** (Go)

- EmpfÃ¤ngt EnrichedEvent stream
- Aggregiert (count by type/user, sum, avg)
- Windowing (z.B. pro 1M events)
- Schreibt Zwischenergebnisse
- **Sprach-Rationale:** Go zeigt exzellente Concurrency (goroutines fÃ¼r parallele aggregation), niedrigen Memory-Footprint, schnelles Compile+Deploy, und ist ein guter Mittelweg zwischen TS und Rust

**apps/sink-service/** (TypeScript)

- EmpfÃ¤ngt AggregateResult stream
- Schreibt finale Outputs
- Metrics reporting

**4.2 Orchestration**

- Supervisor startet alle Services
- Broker verbindet Services (klar definieren: gRPC-Pipeline vs Message-Broker)
- Health monitoring per service
- Restart policy

**4.3 Flow Control & Delivery**

- Backpressure-Regeln (queue limits, max in-flight)
- Drop/Slow-Policy bei Overload
- Fairness: gleiche Batch-Size und windowing wie im Monolith

---

### Phase 5: Metrics & Benchmarking

**5.1 Metrics Collector**

```typescript
// packages/pipeline-metrics/src/collector.ts

export class MetricsCollector {
  // Latenz pro Event/Batch (histogram)
  recordLatency(durationMs: number): void

  // Throughput tracking
  recordProcessed(count: number): void

  // Resource usage (via process.cpuUsage(), process.memoryUsage())
  recordResources(): void

  // Export
  getSnapshot(): PipelineMetrics
  exportPrometheus(): string
}
```

**5.2 Benchmark Harness**

```typescript
// apps/benchmark/src/runner.ts

export interface BenchmarkResult {
  mode: 'monolith' | 'split'
  config: PipelineConfig
  duration: BenchmarkDuration
  metrics: PipelineMetrics
  stability: StabilityMetrics
}

export interface StabilityMetrics {
  restartCount: number
  failureRecoveryTimeMs: number
  dataLoss: number
}

// LÃ¤uft beide Modi mit gleichen Inputs
// Vergleicht side-by-side
// Generiert Report (Markdown + JSON)
```

**5.3 Messungen**

- **p50/p95/p99 Latenz** pro Event oder Batch
- **Durchsatz** (events/sec, MB/sec)
- **CPU-Auslastung** (gesamt + per process/worker)
- **Memory** (RSS peak, heap)
- **Recovery time**: kill random service â†’ Zeit bis wieder grÃ¼n

**5.4 Methodik**

- Warmup-Phase + mehrere LÃ¤ufe (median + Varianz)
- CPU-Pinning / gleiches Load-Profil
- I/O-Cache-Reset oder definierter Zustand
- Identische Configs (batch size, workers/services)

**5.5 Timing & Tracing**

- Monotonic clock fÃ¼r Latenz
- E2E-Latenzdefinition (ingest â†’ sink)
- Trace-ID pro Event fÃ¼r Debug/Replay

---

### Phase 6: Recovery/Restart Demos

**6.1 Chaos Testing**

```typescript
// apps/chaos-monkey/src/chaos.ts

export class ChaosMonkey {
  // Kill random service
  async killRandomService(): Promise<void>

  // Introduce latency
  async addNetworkLatency(ms: number): Promise<void>

  // Stress CPU
  async stressCPU(percent: number): Promise<void>
}
```

**6.2 Demo-Szenarien**

1. **Service Crash:** Parse-Service stirbt â†’ Broker queued events â†’ parse restarts â†’ continues
2. **Network Blip:** TemporÃ¤re Latenz â†’ backpressure â†’ recovery
3. **Resource Exhaustion:** Aggregate-Service OOM â†’ restart â†’ catch up from checkpoint
4. **Rolling Update:** Parse-Service neue Version â†’ alte requests laufen aus â†’ neue Ã¼bernehmen

**6.3 Checkpointing & Replay**

- Checkpoints fÃ¼r Aggregation (interne State-Snapshots)
- Replay-Strategie (offsets/sequence + dedup)
- Datenverlust-Definition (0? maximal toleriert?)

**Monolith Vergleich:**

- Worker Thread crash â†’ oft ganzer Prozess stirbt
- Keine automatische Recovery
- State geht verloren

---

### Phase 7: UI/Visualisierung

**7.1 Supervisor UI Extension**

```
apps/supervisor/src/components/
  PipelineView.tsx       # Overview
  ServiceHealth.tsx      # Health cards pro service
  MetricsChart.tsx       # Real-time throughput/latency
  EventFlow.tsx          # Pipeline flow diagram
  ComparisonView.tsx     # Side-by-side monolith vs split
```

**7.2 Features**

- Real-time metrics (WebSocket)
- Start/Stop services individual
- Chaos actions (kill service button)
- Logs streaming
- Comparison charts (latency, throughput)

---

### Phase 8: Dokumentation

**8.1 README**

- Projekt-Ãœbersicht
- Quick Start (beide Modi)
- Architektur-Diagramme
- Benchmark-Ergebnisse

**8.2 Comparison Report**

```markdown
## Benchmark Results

### Test Setup

- Input: 10M NDJSON events (1.2 GB)
- Machine: MacBook Pro M2, 16GB RAM
- Config: 4 workers/services

### Latency

|       | p50   | p95   | p99   |
| ----- | ----- | ----- | ----- |
| Mono  | 0.8ms | 2.1ms | 5.3ms |
| Split | 1.2ms | 2.8ms | 6.1ms |

**Interpretation:** Split ist 40% langsamer bei p50, aber immer noch < 2ms â†’ fÃ¼r 99% Use Cases irrelevant

### Throughput

- Monolith: 420k events/sec
- Split: 380k events/sec

**Interpretation:** 10% weniger Durchsatz, aber...

### Stability/Recovery

|                    | Monolith | Split   |
| ------------------ | -------- | ------- |
| Service crash      | ðŸ’¥ total | âœ… 1.2s |
| Worker crash       | ðŸ’¥ total | n/a     |
| Recovery time      | manual   | auto    |
| Data loss on crash | ~50k     | 0       |

**Interpretation:** Split ist massiv robuster

### Complexity (LoC, Modules)

- Monolith: 850 LoC, 1 package, komplexe Locks
- Split: 1200 LoC, 5 services, simple contracts

**Interpretation:** 40% mehr Code, aber jeder Service ist simpler
```

**8.3 Argument Guide**

```markdown
## Argumente fÃ¼r Architekten

### "IPC ist doch langsam!"

â†’ Zeige Latenz-Zahlen: < 2ms fÃ¼r 95% der FÃ¤lle
â†’ Zeige dass Parsing/Aggregation dominiert, nicht IPC
â†’ Control plane vs data plane: groÃŸe Daten kÃ¶nnen per file/fd gehen

### "C++ muss bleiben!"

â†’ Zeige Monolith IN C++ vs Split mit Rust/Go/Python/TS
â†’ Rust Parse-Service ist schneller/sicherer als C++ (serde vs rapidjson)
â†’ Go Aggregate ist einfacher als C++ std::thread Chaos
â†’ Python Rules ist produktiver als C++ fÃ¼r Businesslogik
â†’ C++ kann fÃ¼r echte Hot Paths bleiben (SIMD, Echtzeit), aber nicht fÃ¼r alles
â†’ **Kern-Argument:** C++ Monolith verliert gegen moderne Split-Architektur in Robustheit, Wartbarkeit, ProduktivitÃ¤t - und ist kaum schneller

### "Threads sind einfacher!"

â†’ Zeige Lock-KomplexitÃ¤t im Monolith
â†’ Zeige Debuggability im Split (logs per service)
â†’ Zeige Recovery-Demo (kill service â†’ auto restart)

### "Zu viele Sprachen!"

â†’ Zeige dass jede Sprache Vorteile hat (Go fast, Python flexibel, TS produktiv)
â†’ Zeige dass Contracts (Proto) Typsicherheit geben
â†’ Zeige dass Teams parallel arbeiten kÃ¶nnen
```

---

### Phase 9: Testing & Validation

**9.1 Testplan**

- Unit-Tests fÃ¼r Parser/Rules/Aggregation
- Integration-Tests fÃ¼r Pipeline-Ende-zu-Ende
- Golden Outputs + Checksums
- Property-based Tests fÃ¼r Parser (optional)

---

## Repo Structure (neu)

```
apps/
  event-generator/           # NDJSON generator
  event-pipeline-monolith/   # Worker threads variant
  ingest-service/            # Split: read + stream
  parse-service/             # Split: parse (Go/Rust)
  rules-service/             # Split: filter/enrich
  aggregate-service/         # Split: aggregation
  sink-service/              # Split: output
  benchmark/                 # Benchmark runner
  chaos-monkey/              # Chaos testing
  supervisor/                # (erweitert) UI

packages/
  pipeline-common/           # Shared types
  pipeline-metrics/          # Metrics collector
  proto/
    pipeline/
      v1/
        pipeline.proto       # Pipeline services

examples/
  demo-scenarios/            # Recovery demos + scripts
```

---

## Implementation Order

### Sprint 1: Foundation (1-2 Tage)

1. [x] Proto definitions
2. [x] Shared types package
3. [x] Event generator (basic)

### Sprint 2: Monolith (2-3 Tage)

4. [x] Monolith implementation
5. [x] Basic metrics
6. [x] CLI + config

### Sprint 3: Split Services (3-4 Tage)

7. [x] Ingest service (TypeScript)
8. [x] Parse service (Rust + TypeScript fallback)
9. [x] Rules + aggregate + sink (Python + Go + TypeScript fallback)
10. [x] Integration mit Broker (pipeline wiring complete, orchestrator created)

### Sprint 4: Benchmarking (2 Tage)

11. [x] Metrics collector
12. [x] Benchmark harness
13. [x] First results

### Sprint 5: Batching Optimization (1-2 Tage, optional)

Details in section "Sprint 5: Batching Optimization" below.

### Sprint 6: UI + Demos (2-3 Tage)

14. Supervisor UI extension
15. Chaos testing
16. Recovery demos

### Sprint 7: Documentation (1-2 Tage)

17. README + comparison report
18. Argument guide
19. Diagrams

### Sprint 8: Testing & Validation (1-2 Tage)

20. Unit + integration tests
21. Golden outputs + checksums

### Sprint 9: Compute-Heavy Workloads (Optional Extension)

Details in section "Sprint 9: Compute-Heavy Workloads (Optional Extension)" below.

---

## Success Metrics

âœ… **Technisch:**

- Beide Modi laufen mit gleichem Input
- Outputs sind identisch (checksum)
- Messungen sind reproduzierbar
- Recovery-Demo funktioniert zuverlÃ¤ssig

âœ… **Kommunikation:**

- Latenz-Unterschied ist < 2x
- Durchsatz-Unterschied ist < 20%
- Recovery-Zeit ist < 5s
- Architekt versteht: IPC ist nicht das Problem

âœ… **Codebase:**

- Proto breaking change check funktioniert
- Linting + typecheck clean
- Tests vorhanden (basic)
- Dokumentation vollstÃ¤ndig

---

## Optional: Erweiterungen

- **Data Plane Optimization:** Zeige shared memory fÃ¼r sehr groÃŸe Payloads
- **Distributed:** Zeige dass Services auf verschiedenen Maschinen laufen kÃ¶nnen
- **Language Comparison Benchmark:** Parse-Service in allen 3 Sprachen (TS vs Go vs Rust) implementieren und vergleichen â†’ zeigt dass man optimal wÃ¤hlen kann
- **C++ Service Integration:** Zeige einen hypothetischen C++ Service fÃ¼r SIMD-Operationen (z.B. fast checksum/hash) â†’ zeigt dass C++ seinen Platz behÃ¤lt
- **Real-world Input:** Log files von echten Systemen
- **Grafana/Prometheus:** Richtige Observability stack

---

## Polyglot-Strategie (Zusammenfassung)

### Sprach-Matrix

| Service      | Sprache    | Warum                                         | Zeigt                               |
| ------------ | ---------- | --------------------------------------------- | ----------------------------------- |
| **Monolith** | **C++**    | **Legacy-RealitÃ¤t, max Performance, Threads** | **"C++ ist nÃ¶tig" entkrÃ¤ften**      |
| Ingest       | TypeScript | I/O-Streams, Node ecosystem, schnell          | Orchestrierung muss nicht C++       |
| Parse        | Rust       | CPU-intensive, serde_json, memory-safe        | C++-Alternative, schneller/sicherer |
| Rules        | Python     | Flexible Logik, groÃŸe Libs, iteration         | Businesslogik ProduktivitÃ¤t         |
| Aggregate    | Go         | Concurrency (goroutines), low footprint       | Moderne System-Sprache              |
| Sink         | TypeScript | I/O, Metrics, UI-Integration                  | Consistency mit Ingest              |
| Generator    | TypeScript | CLI-Tool, reproduzierbar                      | Tooling-ProduktivitÃ¤t               |
| Benchmark    | TypeScript | Orchestrierung, Report-Gen                    | Cross-language Orchestrierung       |

### Key Messages

1. **C++ Monolith als Baseline** â†’ "Beste" Single-Process Performance, aber komplex/fragil
2. **Split ist kompetitiv trotz IPC** â†’ Latenz-Overhead < 2x, aber massiv robuster
3. **Rust statt C++** â†’ Memory safety + Performance ohne C++ KomplexitÃ¤t, oft schneller als C++ in Praxis
4. **Python fÃ¼r Logik** â†’ Nicht alles braucht maximale Speed, ProduktivitÃ¤t > Mikrooptimierung
5. **Go fÃ¼r Concurrency** â†’ Goroutines vs std::thread - einfacher, sicherer, weniger Boilerplate
6. **TypeScript als Glue** â†’ Produktiv, type-safe, groÃŸes Ecosystem - Orchestrierung muss nicht C++ sein
7. **Polyglot ist kein Chaos** â†’ Proto/gRPC gibt uns Contracts, CI prÃ¼ft, Services sind fokussiert

### Gegen-Argumente entkrÃ¤ften

**"Zu viele Sprachen = Chaos"**
â†’ Proto gibt uns Contracts, CI prÃ¼ft KompatibilitÃ¤t, jeder Service ist klein + fokussiert

**"Wir haben keine Rust/Go Entwickler"**
â†’ Services sind klein (200-400 LoC), gut dokumentiert, onboarding-friendly  
â†’ Alternative: alle TS/Go, aber dann fehlt das "C++-Alternative" Argument

**"Build-System wird komplex"**
â†’ Monorepo mit pnpm, cargo, go mod lÃ¤uft parallel, ist in CI bereits etabliert

**"Debugging wird schwer"**
â†’ Distributed Tracing (Trace-IDs), strukturierte Logs, bessere Isolation als Monolith  
â†’ C++ Monolith debugging ist auch schwer: Race conditions, Deadlocks, Memory corruption

**"C++ ist am schnellsten"**
â†’ Zeige: Rust Parse ist schneller/sicherer als C++ rapidjson  
â†’ Go Concurrency ist einfacher als C++ std::thread ohne Performance-Verlust  
â†’ C++ Overhead durch Locks/Contention kann hÃ¶her sein als IPC-Overhead

---

## Sprint 5: Batching Optimization

**Status:** In Progress  
**Goal:** Reduce IPC overhead from 85-94% to <30% through batching

### Motivation

Current metrics show high IPC overhead:

- **Ingest:** 89% IPC Send (one-at-a-time streaming)
- **Parse:** 94% IPC Recv (waiting for individual events)
- **Aggregate:** 99% IPC Recv (waiting)

**Root cause:** Streaming one event at a time â†’ 100k round trips for 100k events

**Solution:** Batch events to reduce round trips â†’ 1k batches of 100 events = 1k round trips

### Expected Impact

With batch_size=100:

- **Round trips:** 100,000 â†’ 1,000 (100x reduction)
- **IPC overhead:** 85-94% â†’ 25-30% (3x reduction)
- **Throughput:** 30k events/sec â†’ 70-100k events/sec (2-3x improvement)
- **Latency:** Bounded by batch timeout (e.g., 10ms max)

### Implementation Plan

#### 5.1 Proto Changes

```protobuf
// packages/proto/pipeline/v1/pipeline.proto

// Add batch wrapper
message EventBatch {
  repeated RawData events = 1;
  int32 batch_size = 2;
}

message ParsedEventBatch {
  repeated ParsedEvent events = 1;
}

message EnrichedEventBatch {
  repeated EnrichedEvent events = 1;
}

// Update services to use batches
service Parse {
  rpc ParseEvents(stream EventBatch) returns (stream ParsedEventBatch);
}

service Rules {
  rpc ApplyRules(stream ParsedEventBatch) returns (stream EnrichedEventBatch);
}

service Aggregate {
  rpc Aggregate(stream EnrichedEventBatch) returns (stream AggregateResult);
}
```

#### 5.2 Ingest Service (TypeScript)

```typescript
// apps/ingest-service/src/ingest-service.ts

async function* batchEvents(
  events: AsyncIterable<RawData>,
  batchSize: number,
  maxWaitMs: number = 10
): AsyncGenerator<EventBatch> {
  let batch: RawData[] = []
  let batchStart = Date.now()

  for await (const event of events) {
    batch.push(event)

    // Flush if batch full or timeout
    const shouldFlush = batch.length >= batchSize || Date.now() - batchStart > maxWaitMs

    if (shouldFlush) {
      yield { events: batch, batch_size: batch.length }
      batch = []
      batchStart = Date.now()
    }
  }

  // Flush remaining
  if (batch.length > 0) {
    yield { events: batch, batch_size: batch.length }
  }
}

// In StreamEvents RPC
for await (const batch of batchEvents(readEvents(file), 100)) {
  yield batch
}
```

#### 5.3 Parse Service (Rust)

```rust
// apps/parse-service-rust/src/main.rs

async fn parse_events(
    &self,
    request: Request<tonic::Streaming<EventBatch>>,
) -> Result<Response<Self::ParseEventsStream>, Status> {
    let mut stream = request.into_inner();

    let output = async_stream::stream! {
        while let Some(batch) = stream.next().await {
            let batch = batch?;
            let process_start = Instant::now();

            // Process all events in batch
            let parsed: Vec<ParsedEvent> = batch.events
                .par_iter() // Parallel processing with rayon
                .filter_map(|raw| parse_event(&raw.event))
                .collect();

            metrics.record_processing(process_start.elapsed());

            yield Ok(ParsedEventBatch { events: parsed });
        }
    };

    Ok(Response::new(Box::pin(output)))
}
```

#### 5.4 Rules Service (Python)

```python
# apps/rules-service-python/src/rules_service.py

def ApplyRules(self, request_iterator, context):
    for batch in request_iterator:
        process_start = time.perf_counter()

        # Process batch
        enriched = []
        for event in batch.events:
            if event.type == "view":
                continue  # Filter
            enriched.append(enrich_event(event))

        self.metrics.record_processing((time.perf_counter() - process_start) * 1000)

        yield EnrichedEventBatch(events=enriched)
```

#### 5.5 Aggregate Service (Go)

```go
// apps/aggregate-service-go/main.go

func (s *aggregateServer) Aggregate(stream pb.Aggregate_AggregateServer) error {
    for {
        batch, err := stream.Recv()
        if err == io.EOF {
            return s.sendResults(stream)
        }
        if err != nil {
            return err
        }

        processStart := time.Now()

        // Process batch
        for _, event := range batch.Events {
            s.aggregateBatch[event.Type].Count++
            s.aggregateBatch[event.Type].Sum += event.Value
        }

        metrics.recordProcessing(time.Since(processStart).Seconds() * 1000)
    }
}
```

### Testing Strategy

```bash
# Test with different batch sizes
pnpm test:pipeline --batch-size=1    # Baseline (current)
pnpm test:pipeline --batch-size=10   # Small batches
pnpm test:pipeline --batch-size=100  # Optimal
pnpm test:pipeline --batch-size=1000 # Large batches

# Measure throughput
node run-split-pipeline.mjs 100000 --batch-size=100
# Expected: 70-100k events/sec (vs 30k baseline)
```

### Success Metrics

- âœ… IPC overhead reduces from 85-94% to 25-30%
- âœ… Throughput increases from 30k to 70-100k events/sec
- âœ… Latency stays bounded (<50ms p99)
- âœ… All services handle batches correctly
- âœ… Metrics still accurate

### Rollout Plan

1. Implement proto changes + regenerate
2. Update services one-by-one (TS â†’ Rust â†’ Python â†’ Go)
3. Add batch_size CLI flag (default=100)
4. Test with 100k events
5. Compare metrics before/after
6. Document results in RESULTS.md

**Estimate:** 8-12 hours total

---

## Sprint 9: Compute-Heavy Workloads (Optional Extension)

**Status:** Planning (not yet started)  
**Goal:** Add CPU-intensive workload mode to shift focus from IPC to processing

### Current State Analysis

Event pipeline is **I/O-bound** (real measurements from 1k events):

- **Ingest (TS):** 89% IPC Send, 10% processing (0.2Î¼s/event)
- **Parse (Rust):** 94% IPC Recv (waiting!), 5% processing (3.5Î¼s/event)
- **Rules (Python):** 70% processing, 30% IPC (3.3Î¼s/event) â† Best ratio!
- **Aggregate (Go):** 99% IPC Recv (waiting!), 0.1% processing (0.1Î¼s/event)
- **Sink (TS):** 83% IPC Send, 11% processing

**Problem:** Language performance comparison is limited when most time is spent waiting/transferring.

**Solution:** Add compute-heavy workload mode where processing dominates over IPC.

### Design Principles

1. **Non-breaking:** Existing event pipeline remains default
2. **Opt-in:** Activated via CLI flags
3. **Same infrastructure:** Reuses metrics, orchestrator, services
4. **Dual-path:** Services handle both events and work-items

### Workload Design

**Example WorkItem payload:**

```json
{
  "type": "work-item",
  "id": "w-000123",
  "payload": {
    "vectors": [
      [0.12, 0.44, 0.93, 0.21],
      [0.51, 0.09, 0.33, 0.77]
    ],
    "matrix": [
      [1.1, 0.2],
      [0.4, 0.9]
    ],
    "text": "Lorem ipsum dolor sit amet...",
    "iterations": 500
  }
}
```

**Per-service compute tasks:**

- **Parse (Rust):** Vector validation, matrix transpose, preprocessing
- **Rules (Python):** Feature engineering (normalization, thresholding, filtering)
- **Aggregate (Go):** Numeric aggregation (dot products, matrix multiply, reduce/map)
- **Sink (TS):** Result checksumming, JSON formatting

### Implementation Tasks

#### 9.1 Proto Extensions

Add to `packages/proto/pipeline/v1/pipeline.proto`:

```protobuf
message StreamEventsRequest {
  string input_file = 1;
  int32 batch_size = 2;
  string max_events = 3;

  // NEW: Workload mode
  WorkloadMode mode = 4;
  WorkloadConfig config = 5;
}

enum WorkloadMode {
  EVENTS = 0;       // Default: existing event pipeline
  WORK_ITEMS = 1;   // Compute-heavy workloads
  MIXED = 2;        // Mix of both
}

message WorkloadConfig {
  float work_ratio = 1;        // 0.0-1.0: ratio of work-items vs events
  PayloadSize payload_size = 2;
  int32 compute_iterations = 3;
}

enum PayloadSize {
  SMALL = 0;   // 1KB
  MEDIUM = 1;  // 10KB
  LARGE = 2;   // 100KB
}

message WorkItem {
  string id = 1;
  repeated Vector vectors = 2;
  Matrix matrix = 3;
  string text = 4;
  int32 iterations = 5;
}

message Vector {
  repeated double values = 1;
}

message Matrix {
  repeated Vector rows = 1;
}

// Extend RawData
message RawData {
  oneof payload {
    Event event = 1;          // Existing
    WorkItem work_item = 2;   // NEW
  }
}
```

#### 9.2 Service Updates (Dual-Path Pattern)

**TypeScript (Ingest):** Add work item generator

```typescript
function generateWorkItem(id: string, config: WorkloadConfig): WorkItem {
  const size = config.payload_size === 'LARGE' ? 1000 : config.payload_size === 'MEDIUM' ? 100 : 10

  return {
    id,
    vectors: Array.from({ length: 2 }, () => Array.from({ length: size }, () => Math.random())),
    matrix: {
      rows: Array.from({ length: size }, () => Array.from({ length: size }, () => Math.random())),
    },
    text: 'Lorem ipsum '.repeat(config.payload_size === 'LARGE' ? 1000 : 10),
    iterations: config.compute_iterations || 500,
  }
}
```

**Rust (Parse):** Add vector/matrix operations

```rust
fn process_work_item(item: &WorkItem) -> ProcessedWorkItem {
  let process_start = Instant::now();

  // Vector normalization
  let normalized: Vec<Vec<f64>> = item.vectors.iter()
    .map(|v| normalize_vector(&v.values))
    .collect();

  // Matrix transpose
  let transposed = transpose_matrix(&item.matrix);

  // CPU work
  let mut result = 0.0;
  for _ in 0..item.iterations {
    result += compute_hash(&normalized);
  }

  metrics.record_processing(process_start.elapsed().as_secs_f64() * 1000.0);

  ProcessedWorkItem { id: item.id.clone(), vectors: normalized, checksum: result }
}
```

**Python (Rules):** Add numpy/sklearn operations

```python
import numpy as np
from sklearn.preprocessing import StandardScaler

def process_work_item(item: WorkItem) -> EnrichedWorkItem:
    start = time.perf_counter()

    # Feature engineering
    vectors = np.array([v.values for v in item.vectors])
    scaler = StandardScaler()
    normalized = scaler.fit_transform(vectors)

    # Matrix eigenvalues
    matrix = np.array([[c for c in r.values] for r in item.matrix.rows])
    eigenvalues = np.linalg.eigvals(matrix)

    # CPU iterations
    result = sum(np.sum(eigenvalues) for _ in range(item.iterations))

    self.metrics.record_processing((time.perf_counter() - start) * 1000)

    return EnrichedWorkItem(id=item.id, eigenvalues=eigenvalues.tolist(), score=float(result))
```

**Go (Aggregate):** Add numeric operations

```go
func processWorkItem(item *WorkItem, metrics *ServiceMetrics) *AggregateResult {
  start := time.Now()

  // Dot products
  var dotProducts []float64
  for i := 0; i < len(item.Vectors)-1; i++ {
    dp := dotProduct(item.Vectors[i].Values, item.Vectors[i+1].Values)
    dotProducts = append(dotProducts, dp)
  }

  // Matrix sum
  var sum float64
  for _, row := range item.Matrix.Rows {
    for _, val := range row.Values {
      sum += val
    }
  }

  // CPU work
  result := 0.0
  for i := 0; i < int(item.Iterations); i++ {
    result += sum * float64(i)
  }

  metrics.recordProcessing(time.Since(start).Seconds() * 1000)

  return &AggregateResult{Key: item.Id, Sum: int64(result), Avg: result}
}
```

#### 9.3 CLI Extensions

```bash
# Work items only
node run-split-pipeline.mjs 10000 --workload=work-items --payload-size=medium

# Mixed mode: 30% work items, 70% events
node run-split-pipeline.mjs 100000 --workload=mixed --work-ratio=0.3

# CPU-intensive
node run-split-pipeline.mjs 1000 --workload=work-items --iterations=10000
```

#### 9.4 Expected Metrics

Separate output for events vs work-items:

```
=== Pipeline Metrics (Events) ===
Processed: 70,000 events
IPC overhead: 85% (current baseline)

=== Pipeline Metrics (Work Items) ===
Processed: 30,000 work-items
IPC overhead: 25% â† Much lower!
Processing time: 70% â† Language matters here!

=== Language Performance ===
Rust Parse:     3.2ms/item (vector ops)
Python Rules:   8.5ms/item (numpy/sklearn)
Go Aggregate:   1.1ms/item (concurrency)
TS Ingest/Sink: 0.5ms/item (I/O bound)
```

### Decision Point

**Do we implement this?**

**Arguments FOR:**

- Makes language comparison more meaningful
- Shows polyglot advantages in real compute scenarios
- Demonstrates mixed workload capability

**Arguments AGAINST:**

- Current metrics already show Python is fast (70% processing)
- Adds complexity to demo
- Event pipeline is realistic use case
- Could be separate demo

**Recommendation:** Defer to Sprint 9 or create as extension demo.

### Implementation Estimate

- Proto changes: 2h
- Ingest service: 2h
- Parse (Rust): 4h
- Rules (Python + numpy): 4h
- Aggregate (Go): 3h
- Sink: 1h
- Generator: 2h
- Testing: 4h

**Total:** ~22 hours

---

## Commit Message

```
feat(demo): add C++ monolith vs polyglot split pipeline comparison

- Add pipeline proto definitions (ingest/parse/rules/aggregate)
- Implement C++ monolith with std::thread, locks, manual memory management
- Implement split variant: TS (ingest/sink), Rust (parse), Python (rules), Go (aggregate)
- Add benchmarking harness with p95/throughput/recovery/complexity metrics
- Extend supervisor UI for pipeline visualization
- Add chaos testing and recovery demos
- Document comparison showing IPC is not a bottleneck

Shows that C++ monolith loses to modern polyglot split architecture
in robustness, maintainability, and developer productivity - while
being barely faster. Rust provides memory safety + performance,
Python enables rapid iteration, Go simplifies concurrency, TypeScript
boosts orchestration productivity.
```

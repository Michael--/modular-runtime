# Demo Plan: Monolith vs. Split-Architektur

**Ziel:** Zeigen, dass IPC kein Bottleneck ist und Split-Architekturen Vorteile bei KomplexitÃ¤t, Robustheit und Polyglot-FÃ¤higkeit bieten.

## Use Case: NDJSON Event Pipeline

Viele Events verarbeiten â†’ Parse â†’ Regeln â†’ Aggregation â†’ Output

**Warum dieser Use Case:**

- Realistische "viele Daten" Situation
- Streaming-friendly
- Interne Kommunikation ist dominant (wie in Monolithen)
- IPC-Overhead ist messbar aber nicht dominant
- Recovery/Restart-Szenarien sind Ã¼berzeugend

---

## Architektur-Vergleich

### Monolith (C++ mit std::thread)

```
Main Thread (Orchestrator)
  â”œâ”€ Reader Thread (NDJSON laden, std::ifstream)
  â”œâ”€ Parser Threads (parse + validate, rapidjson/simdjson)
  â”œâ”€ Rules Thread (filter/enrich, custom logic)
  â”œâ”€ Aggregator Thread (counts/windows, std::unordered_map)
  â””â”€ Writer Thread (output, std::ofstream)

Kommunikation: std::queue + std::mutex, std::condition_variable
Problem: Locks Ã¼berall, komplexe Synchronisation, shared state, manuelle Memory-Management

**Warum C++?**
- ReprÃ¤sentiert typischen Legacy-Monolith ("C++ ist nÃ¶tig")
- Performance-Baseline: beste mÃ¶gliche Single-Process Performance
- Zeigt reale KomplexitÃ¤t: Locks, RAII, Thread-Safety, Memory-Leaks
- Macht Vergleich Ã¼berzeugend: "Selbst gegen C++ ist Split kompetitiv"
```

### Split (Services + Broker)

```
Broker
  â”œâ”€ ingest-service (TypeScript) - liest NDJSON, chunked streaming, Node.js streams
  â”œâ”€ parse-service (Rust) - parse + validate, serde_json, maximale Performance
  â”œâ”€ rules-service (Python) - filter/enrich, dynamische Regeln, schnelle Iteration
  â”œâ”€ aggregate-service (Go) - counts/windows, goroutines, efficient concurrency
  â””â”€ sink-service (TypeScript) - write results, metrics reporting

Kommunikation: gRPC/Protobuf
Vorteile: Prozessgrenzen, klare Contracts, polyglot (4 Sprachen!), restartable

Polyglot-Strategie:
- TypeScript: Orchestrierung, I/O-heavy, schnelle Entwicklung
- Rust: CPU-intensive parsing, memory safety, C++-Alternative
- Python: Flexible Businesslogik, schnelle Iteration, groÃŸe Libs
- Go: Concurrency-heavy aggregation, deployment-friendly
```

---

## Implementierungsplan

### Phase 1: Foundation (Proto + Shared Types)

**1.1 Proto Definitions**

```protobuf
// packages/proto/pipeline/v1/pipeline.proto

service Ingest {
  rpc StreamEvents(StreamEventsRequest) returns (stream Event);
  rpc GetStatus(GetStatusRequest) returns (IngestStatus);
}

service Parse {
  rpc ParseEvents(stream Event) returns (stream ParsedEvent);
}

service Rules {
  rpc ApplyRules(stream ParsedEvent) returns (stream EnrichedEvent);
}

service Aggregate {
  rpc Aggregate(stream EnrichedEvent) returns (stream AggregateResult);
}

message Event {
  string raw_json = 1;
  int64 sequence = 2;
}

message ParsedEvent {
  string type = 1;
  string user = 2;
  int64 value = 3;
  int64 timestamp = 4;
  int64 sequence = 5;
}

message EnrichedEvent {
  ParsedEvent event = 1;
  map<string, string> metadata = 2;
  bool passed_rules = 3;
}

message AggregateResult {
  string key = 1;
  int64 count = 2;
  int64 sum = 3;
  double avg = 4;
}
```

**1.2 Shared Package**

```typescript
// packages/pipeline-common/src/types.ts

export interface EventRecord {
  ts: string
  type: 'click' | 'view' | 'purchase'
  user: string
  value: number
  metadata?: Record<string, unknown>
}

export interface PipelineMetrics {
  processedEvents: number
  invalidEvents: number
  throughputPerSec: number
  latencyP50Ms: number
  latencyP95Ms: number
  latencyP99Ms: number
  cpuPercent: number
  memoryMB: number
}

export interface PipelineConfig {
  mode: 'monolith' | 'split'
  inputFile: string
  outputFile: string
  batchSize: number
  workerCount?: number // monolith only
}
```

**1.3 Contract Semantics (Ã¼bergreifend)**

- Ordering: garantiert pro `sequence`/`key` oder nur best-effort?
- Delivery: at-least-once vs exactly-once (und was ist im Demo-Context akzeptabel)
- Idempotenz/Dedup: wie werden Retries gehandhabt, welche IDs sind stabil
- Ack/Retry Regeln: max retries, backoff, retry budget

**1.4 Schema Evolution**

- Proto-Versionierung (v1/v2) + KompatibilitÃ¤tsregeln
- CI-Check fÃ¼r breaking changes (Protos)
- Beispiel fÃ¼r additive Ã„nderungen + Deprecation-Policy

---

### Phase 2: Test Data Generator

**2.1 Generator Tool**

```typescript
// apps/event-generator/src/generator.ts

export interface GeneratorConfig {
  eventCount: number
  outputFile: string
  userCount: number
  eventTypes: string[]
  seed?: number
}

// Generiert z.B. 5M, 10M, 50M Events
// Reproduzierbar mit seed
// Verschiedene Verteilungen (uniform, zipf, burst)
```

**Beispiel Output:**

```json
{"ts":"2026-01-30T10:00:00Z","type":"click","user":"u1234","value":42}
{"ts":"2026-01-30T10:00:01Z","type":"view","user":"u5678","value":1}
...
```

---

### Phase 3: Monolith Implementation (C++)

**3.1 Structure**

```
apps/event-pipeline-monolith/
  CMakeLists.txt
  src/
    main.cpp              # CLI entry, orchestration
    threads/
      reader.cpp/.hpp     # Read NDJSON file (std::ifstream)
      parser.cpp/.hpp     # Parse + validate (rapidjson/simdjson)
      rules.cpp/.hpp      # Filter/enrich
      aggregator.cpp/.hpp # Count/sum/avg (std::unordered_map + mutex)
      writer.cpp/.hpp     # Output results
    coordinator.cpp/.hpp  # Thread management + queues
    metrics.cpp/.hpp      # Collect metrics (atomics)
    queue.hpp             # Thread-safe queue (std::queue + mutex/cv)
```

**3.2 Thread Communication**

- `std::queue<T>` + `std::mutex` + `std::condition_variable` fÃ¼r Event-Passing
- `std::atomic` fÃ¼r Metrics (lock-free counters)
- Backpressure via queue size limits + cv.wait()
- RAII guards (`std::lock_guard`, `std::unique_lock`) Ã¼berall

**3.3 Key Implementation Points**

- Locks fÃ¼r shared state (aggregation maps): `std::shared_mutex` (reader/writer lock)
- Komplexe Shutdown-Logik: `std::atomic<bool> shutdown_flag`, join() alle threads
- Error handling: try/catch + error queues, keine exceptions Ã¼ber Thread-Grenzen
- Memory management: smart pointers (`std::unique_ptr`, `std::shared_ptr`), aber auch manuelle Cleanup-Logik
- **Typische C++ Probleme zeigen:** Race conditions, Deadlocks, Memory leaks (falls vorhanden), komplexe Debugging

**3.4 Build & Dependencies**

- CMake + vcpkg/conan fÃ¼r Dependencies (rapidjson, CLI11, fmt)
- Compiler: clang++ oder g++ mit C++17/20
- Sanitizers: -fsanitize=thread fÃ¼r Tests (zeigt Race conditions)
- Profiling: perf, Instruments (zeigt Lock-Contention)

---

### Phase 4: Split Implementation

**4.1 Services**

**apps/ingest-service/** (TypeScript)

- Liest NDJSON file, streamt in Chunks
- Implementiert `Ingest` gRPC service
- Rate limiting / backpressure
- Health checks

**apps/parse-service/** (Rust)

- EmpfÃ¤ngt Event stream
- Parse JSON â†’ structured data (serde_json)
- Validierung (type-safe)
- Schnell + memory-efficient
- **Sprach-Rationale:** Rust zeigt maximale Performance bei JSON-Parsing (serde), memory safety ohne GC, und ist ein starkes Gegenargument zu "nur C++ ist schnell"

**apps/rules-service/** (Python)

- EmpfÃ¤ngt ParsedEvent stream
- Wendet Regeln an (z.B. filter by type, enrich with geo data)
- Dynamische Regel-Engine (eval/AST oder rule DSL)
- Demonstriert: "Businesslogik muss nicht C++"
- **Sprach-Rationale:** Python zeigt FlexibilitÃ¤t + schnelle Iteration bei Regeln/Heuristiken, groÃŸe Lib-Ecosystem (pandas, numpy falls benÃ¶tigt), und dass Performance-kritische Teile isoliert sein kÃ¶nnen

**apps/aggregate-service/** (Go)

- EmpfÃ¤ngt EnrichedEvent stream
- Aggregiert (count by type/user, sum, avg)
- Windowing (z.B. pro 1M events)
- Schreibt Zwischenergebnisse
- **Sprach-Rationale:** Go zeigt exzellente Concurrency (goroutines fÃ¼r parallele aggregation), niedrigen Memory-Footprint, schnelles Compile+Deploy, und ist ein guter Mittelweg zwischen TS und Rust

**apps/sink-service/** (TypeScript)

- EmpfÃ¤ngt AggregateResult stream
- Schreibt finale Outputs
- Metrics reporting

**4.2 Orchestration**

- Supervisor startet alle Services
- Broker verbindet Services (klar definieren: gRPC-Pipeline vs Message-Broker)
- Health monitoring per service
- Restart policy

**4.3 Flow Control & Delivery**

- Backpressure-Regeln (queue limits, max in-flight)
- Drop/Slow-Policy bei Overload
- Fairness: gleiche Batch-Size und windowing wie im Monolith

---

### Phase 5: Metrics & Benchmarking

**5.1 Metrics Collector**

```typescript
// packages/pipeline-metrics/src/collector.ts

export class MetricsCollector {
  // Latenz pro Event/Batch (histogram)
  recordLatency(durationMs: number): void

  // Throughput tracking
  recordProcessed(count: number): void

  // Resource usage (via process.cpuUsage(), process.memoryUsage())
  recordResources(): void

  // Export
  getSnapshot(): PipelineMetrics
  exportPrometheus(): string
}
```

**5.2 Benchmark Harness**

```typescript
// apps/benchmark/src/runner.ts

export interface BenchmarkResult {
  mode: 'monolith' | 'split'
  config: PipelineConfig
  duration: BenchmarkDuration
  metrics: PipelineMetrics
  stability: StabilityMetrics
}

export interface StabilityMetrics {
  restartCount: number
  failureRecoveryTimeMs: number
  dataLoss: number
}

// LÃ¤uft beide Modi mit gleichen Inputs
// Vergleicht side-by-side
// Generiert Report (Markdown + JSON)
```

**5.3 Messungen**

- **p50/p95/p99 Latenz** pro Event oder Batch
- **Durchsatz** (events/sec, MB/sec)
- **CPU-Auslastung** (gesamt + per process/worker)
- **Memory** (RSS peak, heap)
- **Recovery time**: kill random service â†’ Zeit bis wieder grÃ¼n

**5.4 Methodik**

- Warmup-Phase + mehrere LÃ¤ufe (median + Varianz)
- CPU-Pinning / gleiches Load-Profil
- I/O-Cache-Reset oder definierter Zustand
- Identische Configs (batch size, workers/services)

**5.5 Timing & Tracing**

- Monotonic clock fÃ¼r Latenz
- E2E-Latenzdefinition (ingest â†’ sink)
- Trace-ID pro Event fÃ¼r Debug/Replay

---

### Phase 6: Recovery/Restart Demos

**6.1 Chaos Testing**

```typescript
// apps/chaos-monkey/src/chaos.ts

export class ChaosMonkey {
  // Kill random service
  async killRandomService(): Promise<void>

  // Introduce latency
  async addNetworkLatency(ms: number): Promise<void>

  // Stress CPU
  async stressCPU(percent: number): Promise<void>
}
```

**6.2 Demo-Szenarien**

1. **Service Crash:** Parse-Service stirbt â†’ Broker queued events â†’ parse restarts â†’ continues
2. **Network Blip:** TemporÃ¤re Latenz â†’ backpressure â†’ recovery
3. **Resource Exhaustion:** Aggregate-Service OOM â†’ restart â†’ catch up from checkpoint
4. **Rolling Update:** Parse-Service neue Version â†’ alte requests laufen aus â†’ neue Ã¼bernehmen

**6.3 Checkpointing & Replay**

- Checkpoints fÃ¼r Aggregation (interne State-Snapshots)
- Replay-Strategie (offsets/sequence + dedup)
- Datenverlust-Definition (0? maximal toleriert?)

**Monolith Vergleich:**

- Worker Thread crash â†’ oft ganzer Prozess stirbt
- Keine automatische Recovery
- State geht verloren

---

### Phase 7: UI/Visualisierung

**7.1 Supervisor UI Extension**

```
apps/supervisor/src/components/
  PipelineView.tsx       # Overview
  ServiceHealth.tsx      # Health cards pro service
  MetricsChart.tsx       # Real-time throughput/latency
  EventFlow.tsx          # Pipeline flow diagram
  ComparisonView.tsx     # Side-by-side monolith vs split
```

**7.2 Features**

- Real-time metrics (WebSocket)
- Start/Stop services individual
- Chaos actions (kill service button)
- Logs streaming
- Comparison charts (latency, throughput)

---

### Phase 8: Dokumentation

**8.1 README**

- Projekt-Ãœbersicht
- Quick Start (beide Modi)
- Architektur-Diagramme
- Benchmark-Ergebnisse

**8.2 Comparison Report**

```markdown
## Benchmark Results

### Test Setup

- Input: 10M NDJSON events (1.2 GB)
- Machine: MacBook Pro M2, 16GB RAM
- Config: 4 workers/services

### Latency

|       | p50   | p95   | p99   |
| ----- | ----- | ----- | ----- |
| Mono  | 0.8ms | 2.1ms | 5.3ms |
| Split | 1.2ms | 2.8ms | 6.1ms |

**Interpretation:** Split ist 40% langsamer bei p50, aber immer noch < 2ms â†’ fÃ¼r 99% Use Cases irrelevant

### Throughput

- Monolith: 420k events/sec
- Split: 380k events/sec

**Interpretation:** 10% weniger Durchsatz, aber...

### Stability/Recovery

|                    | Monolith | Split   |
| ------------------ | -------- | ------- |
| Service crash      | ðŸ’¥ total | âœ… 1.2s |
| Worker crash       | ðŸ’¥ total | n/a     |
| Recovery time      | manual   | auto    |
| Data loss on crash | ~50k     | 0       |

**Interpretation:** Split ist massiv robuster

### Complexity (LoC, Modules)

- Monolith: 850 LoC, 1 package, komplexe Locks
- Split: 1200 LoC, 5 services, simple contracts

**Interpretation:** 40% mehr Code, aber jeder Service ist simpler
```

**8.3 Argument Guide**

```markdown
## Argumente fÃ¼r Architekten

### "IPC ist doch langsam!"

â†’ Zeige Latenz-Zahlen: < 2ms fÃ¼r 95% der FÃ¤lle
â†’ Zeige dass Parsing/Aggregation dominiert, nicht IPC
â†’ Control plane vs data plane: groÃŸe Daten kÃ¶nnen per file/fd gehen

### "C++ muss bleiben!"

â†’ Zeige Monolith IN C++ vs Split mit Rust/Go/Python/TS
â†’ Rust Parse-Service ist schneller/sicherer als C++ (serde vs rapidjson)
â†’ Go Aggregate ist einfacher als C++ std::thread Chaos
â†’ Python Rules ist produktiver als C++ fÃ¼r Businesslogik
â†’ C++ kann fÃ¼r echte Hot Paths bleiben (SIMD, Echtzeit), aber nicht fÃ¼r alles
â†’ **Kern-Argument:** C++ Monolith verliert gegen moderne Split-Architektur in Robustheit, Wartbarkeit, ProduktivitÃ¤t - und ist kaum schneller

### "Threads sind einfacher!"

â†’ Zeige Lock-KomplexitÃ¤t im Monolith
â†’ Zeige Debuggability im Split (logs per service)
â†’ Zeige Recovery-Demo (kill service â†’ auto restart)

### "Zu viele Sprachen!"

â†’ Zeige dass jede Sprache Vorteile hat (Go fast, Python flexibel, TS produktiv)
â†’ Zeige dass Contracts (Proto) Typsicherheit geben
â†’ Zeige dass Teams parallel arbeiten kÃ¶nnen
```

---

### Phase 9: Testing & Validation

**9.1 Testplan**

- Unit-Tests fÃ¼r Parser/Rules/Aggregation
- Integration-Tests fÃ¼r Pipeline-Ende-zu-Ende
- Golden Outputs + Checksums
- Property-based Tests fÃ¼r Parser (optional)

---

## Repo Structure (neu)

```
apps/
  event-generator/           # NDJSON generator
  event-pipeline-monolith/   # Worker threads variant
  ingest-service/            # Split: read + stream
  parse-service/             # Split: parse (Go/Rust)
  rules-service/             # Split: filter/enrich
  aggregate-service/         # Split: aggregation
  sink-service/              # Split: output
  benchmark/                 # Benchmark runner
  chaos-monkey/              # Chaos testing
  supervisor/                # (erweitert) UI

packages/
  pipeline-common/           # Shared types
  pipeline-metrics/          # Metrics collector
  proto/
    pipeline/
      v1/
        pipeline.proto       # Pipeline services

examples/
  demo-scenarios/            # Recovery demos + scripts
```

---

## Implementation Order

### Sprint 1: Foundation (1-2 Tage)

1. [x] Proto definitions
2. [x] Shared types package
3. [x] Event generator (basic)

### Sprint 2: Monolith (2-3 Tage)

4. [x] Monolith implementation
5. [x] Basic metrics
6. [x] CLI + config

### Sprint 3: Split Services (3-4 Tage)

7. Ingest service
8. Parse service (Go)
9. Rules + aggregate + sink
10. Integration mit Broker

### Sprint 4: Benchmarking (2 Tage)

11. Metrics collector
12. Benchmark harness
13. First results

### Sprint 5: UI + Demos (2-3 Tage)

14. Supervisor UI extension
15. Chaos testing
16. Recovery demos

### Sprint 6: Documentation (1-2 Tage)

17. README + comparison report
18. Argument guide
19. Diagrams

### Sprint 7: Testing & Validation (1-2 Tage)

20. Unit + integration tests
21. Golden outputs + checksums

---

## Success Metrics

âœ… **Technisch:**

- Beide Modi laufen mit gleichem Input
- Outputs sind identisch (checksum)
- Messungen sind reproduzierbar
- Recovery-Demo funktioniert zuverlÃ¤ssig

âœ… **Kommunikation:**

- Latenz-Unterschied ist < 2x
- Durchsatz-Unterschied ist < 20%
- Recovery-Zeit ist < 5s
- Architekt versteht: IPC ist nicht das Problem

âœ… **Codebase:**

- Proto breaking change check funktioniert
- Linting + typecheck clean
- Tests vorhanden (basic)
- Dokumentation vollstÃ¤ndig

---

## Optional: Erweiterungen

- **Data Plane Optimization:** Zeige shared memory fÃ¼r sehr groÃŸe Payloads
- **Distributed:** Zeige dass Services auf verschiedenen Maschinen laufen kÃ¶nnen
- **Language Comparison Benchmark:** Parse-Service in allen 3 Sprachen (TS vs Go vs Rust) implementieren und vergleichen â†’ zeigt dass man optimal wÃ¤hlen kann
- **C++ Service Integration:** Zeige einen hypothetischen C++ Service fÃ¼r SIMD-Operationen (z.B. fast checksum/hash) â†’ zeigt dass C++ seinen Platz behÃ¤lt
- **Real-world Input:** Log files von echten Systemen
- **Grafana/Prometheus:** Richtige Observability stack

---

## Polyglot-Strategie (Zusammenfassung)

### Sprach-Matrix

| Service      | Sprache    | Warum                                         | Zeigt                               |
| ------------ | ---------- | --------------------------------------------- | ----------------------------------- |
| **Monolith** | **C++**    | **Legacy-RealitÃ¤t, max Performance, Threads** | **"C++ ist nÃ¶tig" entkrÃ¤ften**      |
| Ingest       | TypeScript | I/O-Streams, Node ecosystem, schnell          | Orchestrierung muss nicht C++       |
| Parse        | Rust       | CPU-intensive, serde_json, memory-safe        | C++-Alternative, schneller/sicherer |
| Rules        | Python     | Flexible Logik, groÃŸe Libs, iteration         | Businesslogik ProduktivitÃ¤t         |
| Aggregate    | Go         | Concurrency (goroutines), low footprint       | Moderne System-Sprache              |
| Sink         | TypeScript | I/O, Metrics, UI-Integration                  | Consistency mit Ingest              |
| Generator    | TypeScript | CLI-Tool, reproduzierbar                      | Tooling-ProduktivitÃ¤t               |
| Benchmark    | TypeScript | Orchestrierung, Report-Gen                    | Cross-language Orchestrierung       |

### Key Messages

1. **C++ Monolith als Baseline** â†’ "Beste" Single-Process Performance, aber komplex/fragil
2. **Split ist kompetitiv trotz IPC** â†’ Latenz-Overhead < 2x, aber massiv robuster
3. **Rust statt C++** â†’ Memory safety + Performance ohne C++ KomplexitÃ¤t, oft schneller als C++ in Praxis
4. **Python fÃ¼r Logik** â†’ Nicht alles braucht maximale Speed, ProduktivitÃ¤t > Mikrooptimierung
5. **Go fÃ¼r Concurrency** â†’ Goroutines vs std::thread - einfacher, sicherer, weniger Boilerplate
6. **TypeScript als Glue** â†’ Produktiv, type-safe, groÃŸes Ecosystem - Orchestrierung muss nicht C++ sein
7. **Polyglot ist kein Chaos** â†’ Proto/gRPC gibt uns Contracts, CI prÃ¼ft, Services sind fokussiert

### Gegen-Argumente entkrÃ¤ften

**"Zu viele Sprachen = Chaos"**
â†’ Proto gibt uns Contracts, CI prÃ¼ft KompatibilitÃ¤t, jeder Service ist klein + fokussiert

**"Wir haben keine Rust/Go Entwickler"**
â†’ Services sind klein (200-400 LoC), gut dokumentiert, onboarding-friendly  
â†’ Alternative: alle TS/Go, aber dann fehlt das "C++-Alternative" Argument

**"Build-System wird komplex"**
â†’ Monorepo mit pnpm, cargo, go mod lÃ¤uft parallel, ist in CI bereits etabliert

**"Debugging wird schwer"**
â†’ Distributed Tracing (Trace-IDs), strukturierte Logs, bessere Isolation als Monolith  
â†’ C++ Monolith debugging ist auch schwer: Race conditions, Deadlocks, Memory corruption

**"C++ ist am schnellsten"**
â†’ Zeige: Rust Parse ist schneller/sicherer als C++ rapidjson  
â†’ Go Concurrency ist einfacher als C++ std::thread ohne Performance-Verlust  
â†’ C++ Overhead durch Locks/Contention kann hÃ¶her sein als IPC-Overhead

---

## Commit Message

```
feat(demo): add C++ monolith vs polyglot split pipeline comparison

- Add pipeline proto definitions (ingest/parse/rules/aggregate)
- Implement C++ monolith with std::thread, locks, manual memory management
- Implement split variant: TS (ingest/sink), Rust (parse), Python (rules), Go (aggregate)
- Add benchmarking harness with p95/throughput/recovery/complexity metrics
- Extend supervisor UI for pipeline visualization
- Add chaos testing and recovery demos
- Document comparison showing IPC is not a bottleneck

Shows that C++ monolith loses to modern polyglot split architecture
in robustness, maintainability, and developer productivity - while
being barely faster. Rust provides memory safety + performance,
Python enables rapid iteration, Go simplifies concurrency, TypeScript
boosts orchestration productivity.
```
